---
title: "Analysis"
author: ""
format: html
editor: visual
---

Overview of Data

```{r}
library(tidyverse)
library(knitr) 
library(ggfortify)
library(gplots)

load("./data/yeast.RData")

str(y)  # Check the structure of the dataset
summary(y)  # Summary statistics

cor(y$Location_Center_X, y$AreaShape_Center_X) # 100% correlated

data_scaled <- data.frame(scale(y[, sapply(y, is.numeric)])) |> select(!c(Location_Center_X,
                                                              Location_Center_Y))
```

# 1. Grouping/Clustering

The pdf shared in class (also in the data folder) does a better job then I will ever do for this

```{r}
pc_scores <- pca$x[, 1:6]
pc_scores_new <- data.frame(pc_scores) |> rownames_to_column(var = "rowname") |>
  mutate(img = substring(rowname, 0, 8)) |>
  select(!rowname)

pc_scores_sum <- pc_scores_new |> group_by(img) |>
  summarize(PC1.medn = median(PC1), PC2.medn = median(PC2), PC3.medn = median(PC3),
            PC4.medn = median(PC4), PC5.medn = median(PC5), PC6.medn = median(PC6)) |>
  column_to_rownames(var = "img")
dist_matrix <- dist(pc_scores_sum)


hc_result <- hclust(dist_matrix, method = "complete")
plot(hc_result, main = "Hierarchical Clustering on First 6 PCs", xlab = "", sub = "")

heatmap.2(as.matrix(pc_scores_sum),scale="none",trace="none",
          cexCol=1)
```

# 2. Feature Selection/Specification

Using PCA

```{r}
pca <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
sum <- summary(pca)
data.frame(sum$importance) |> select(PC1, PC2, PC3, PC4, PC5, PC6) |> kable()
```

```{r}
biplot(pca, main = "PCA Biplot", cex = 0.6)

# Make pca plot using well?
# Separate loading plot

autoplot(pca, data = y, color = "strain", alpha = 0.5)
```

```{r}
var_explained <- pca$sdev^2 / sum(pca$sdev^2)
sum(var_explained[1:6])

ggplot(data = data.frame(PC = 1:length(var_explained), 
                         Variance = var_explained),
       aes(x = PC, y = Variance)) +
  geom_line() +
  geom_point(size = 4) +
  geom_text(aes(label = round(Variance, 2)), vjust = -0.5) +
  scale_x_continuous(breaks = 1:length(var_explained)) +
  xlab("Principal Component") +
  ylab("Proportion of Variance Explained") +
  ggtitle("Scree Plot") +
  theme_minimal()
```

```{r}
loadings <- abs(pca$rotation[, 1:6])  # Focus on PC1
important_features <- names(sort(loadings, decreasing = TRUE)[1:5])
important_features

loadings_mod <- data.frame(loadings) |> mutate(weighted = PC1 * var_explained[1] +
                                     PC2 * var_explained[2] +
                                     PC3 * var_explained[3] +
                                     PC4 * var_explained[4] +
                                     PC5 * var_explained[5] +
                                     PC6 * var_explained[6])
```

# Outlier Identification

-   Use Mahalanobis Distance?- <https://www.machinelearningplus.com/statistics/mahalanobis-distance/>

    -   It transforms the columns into uncorrelated variables

    -   Scale the columns to make their variance equal to 1

    -   Finally, it calculates the Euclidean distance.

-   If we stick with this, need to find scholarly source for citation (one idea: **APA Citation:** Mahalanobis, P. C. (1936). On the generalised distance in statistics. *Proceedings of the National Institute of Sciences of India,* 2(1), 49â€“55.)

-   Euclidean Distance for now

    ```{r}
    euc_dist <- apply(data_scaled, 1, function(x) sqrt(sum(x^2)))
    threshold <- quantile(euc_dist, 0.99)  # Top 1% as outliers
    outliers <- which(euc_dist > threshold)
    ```

```{r}
y$Outlier <- ifelse(1:nrow(y) %in% outliers, TRUE, FALSE)
```

# 4 - Visualization

```{r}
library(ggplot2)
```

Visualize clusters using first two PCA components

```{r}
pca_data <- data.frame(pca$x[, 1:2], Cluster = y$Cluster)
ggplot(pca_data, aes(PC1, PC2, color = Cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "Cluster Visualization using PCA", x = "PC1", y = "PC2") +
  theme_minimal()
```

Visualize Outliers:

```{r}
outlier_data <- data.frame(y, Euclidean_Distance = euc_dist)
ggplot(outlier_data, aes(x = Euclidean_Distance, fill = Outlier)) +
  geom_histogram(bins = 30, alpha = 0.6) +
  labs(title = "Outlier Detection", x = "Euclidean Distance", y = "Count") +
  theme_minimal()
```

-   The majority of the data has a low Euclidean distance, forming a right-skewed distribution.

-   The blue bars represent detected outliers, which are on the far right, meaning they have significantly higher Euclidean distances compared to the rest.

-   The outliers are few and exist at extreme values, indicating the top 1% threshold successfully isolates anomalies.

-   Causes of outliers?: could be due to measurement errors, rare yeast morphologies, or misclassified samples.

Save cluster assigments / outlier data for later:

```{r}
write.csv(y, "yeast_analysis_results.csv", row.names = FALSE)
```


